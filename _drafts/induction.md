
      <section>
        <h2 id="Induction">Inductive Generalization and Inductive Biases</h2>

        Our goal in building machine learning systems is, with rare exceptions, to create algorithms whose utility extends beyond the dataset in which they are trained. In other words, we desire intelligent systems that are capable of generalizing to future data. The process of leveraging observations to draw inferences about the unobserved is the principle of _induction_<label for="sn-in-his-later-books" class="margin-toggle sidenote-number"></label></span><input type="checkbox" id="sn-in-his-later-books" class="margin-toggle"/><span class="sidenote">Terminological note: In a non-technical setting, the term _inductive_ -- denoting the inference of general laws from particular instances -- is typically contrasted with the adjective _deductive_, which denotes the inference of particular instances from general laws. This broad definition of induction may be used in machine learning to describe, for example, the model fitting process as the _inductive step_ and the deployment on new data as the _deductive step_. By the same token, some AI methods such as automated theorem provers\cite{fitting2012first} are described as deductive. In the setting of current ML research, however, it is much more common for the term `inductive' to refer specifically to methods that are structurally capable of operating on new data points without retraining. In contrast, _transductive_ methods require a fixed or pre-specified dataset, and are used to make internal predictions about missing features or labels\cite{chami2020machine}.While many ML methods are assumed to be inductive in both senses of the term, this section concerns itself primarily with the broader notion of induction as it relates to learning from observed data. In contrast, Chapters \ref{chapter:subgraph} and \ref{chapter:chemGeAlign} involve the second use of this term, as I propose new methods that are inductive but whose predecessors were transductive.</span>.

        <h3 id="Induction_Phil">Philosophical Foundations for the Problem of Induction</h2>
        
          Even ancient philosophers appreciated the tenuity of inductive generalization. As early as the second century, the Greek philosopher Sextus Empiricus argued that the very notion of induction was invalid \cite{empiricus1933outlines}, a conclusion independently argued by the Charvaka school of philosophy in ancient India \cite{perrett1984problem}. The so-called “problem of induction,” as it is best known today, was formulated by 18th-century philosopher David Hume in his twin works _A Treatise of Human Nature_ \cite{humeTreatise} and _An Enquiry Concerning Human Understanding_ \cite{humeEnquiry}. In these works, Hume argues that all inductive inference hinges upon the premise that the future will follow the past. This premise has since become known as his “Principle of Uniformity of Nature” (or simply, the “Uniformity Principle”), the “Resemblance Principle,” or his “Principle of Extrapolation.” \cite{garrett2011reason}
        

          In the _Treatise_ and the _Inquiry_, Hume examines various arguments -- intuitive, demonstrative, sensible, probabilistic -- that could be proposed to establish the principle of extrapolation and, having rejected them all, concludes that inductive inference itself is “not determin’d by reason.” Hume thus places induction outside the scope of reason itself, casting it therefore as non-rational if not irrational. In his 1955 work _Fact, Fiction, and Forecast_, Nelson Goodman extended and reframed Hume's arguments, proposing ``a new riddle of induction."\cite{goodman1955fact} For Goodman, the key challenge was not the validity of induction per se, but rather the recognition that for any set of observations, there are multiple contradictory generalizations that could be used to explain them.
        

          At least among scientists, the best known formal response to the problem of induction comes from the philosopher of science Karl Popper. In _Conjectures and Refutations_\cite{popper2014conjectures}, Popper argues that science may sidestep the problem of induction by relying instead upon scientific conjecture followed by criticism.  Stated otherwise, according to Popper, the central goal of scientists should be to formulate falsifiable theories which can be provisionally treated as true when they survive repeated attempts to prove them false <label for="sn-in-his-later-books" class="margin-toggle sidenote-number"></label></span><input type="checkbox" id="sn-in-his-later-books" class="margin-toggle"/><span class="sidenote">Popper's framing is frequently used to justify the statistical hypothesis testing frameworks proposed by the likes of Neyman, Pearson, and Fisher. However, the compatibility of Popperian falsification and statistical hypothesis testing is a matter of debate. \cite{hilborn1997ecological, mayo1996ducks, queen2002experimental}</span>. Popper's arguments may be helpful as we frame our evaluation of any specific ML system that has already been trained -- and thus instantiated, in a sense, as a ``conjecture" that can be refuted. However, the training process of ML systems is itself an act of inductive inference and thus relies on a Uniformity Principle in a way that Popper's conjecture-refutation framework does not address.
        

         This thesis is not a work of philosophy. However, I consider it important to acknowledge that the entire field of machine learning -- the branch of AI concerned with constructing computers that learn from experience\cite{mitchell1997machine} -- is predicated upon a core premise that has, for centuries, been recognized as unprovable and arguably non-rational. To boot, even if the inductive framework is accepted as valid, there are an infinite number of contradictory generalizations that are equally consistent with our training data. While these observations may be philosophical in spirit and may appear impractical, they provide a framing for extremely practical questions:
        

         Under which circumstances can we reasonably expect the future to resemble the past, as far as our models are concerned? Given an infinite number of valid generalizations from our data -- most of which are presumably useless or even dangerous -- what guiding principles do we leverage to choose between them? What are the falsifiable empirical claims that we should be making about our models, and how should we test them? If we are to assume that prospective failure of our systems is the most likely outcome, as Popper would, what reasonable standards can be set to nevertheless trust ML in safety-critical settings such as healthcare?
        

          Each of these questions will be repeatedly considered throughout the course of this thesis.
        

        <h3 id="Induction_ML">Inductive Biases in Machine Learning</h2>

          As outlined above, the paradigm of machine learning presupposes the identification -- a la Hume -- of some set of tasks and environments for which we expect the future to resemble the past. At this point, we are thus forced to determine guiding principles -- a la Goodman -- that give our models strong _a priori_ preferences for generalizations that we expect to extrapolate well into the future. When such guiding principles are instantiated as design decisions in our models, they are known as _inductive biases_.
        

          In his 1980 report _The Need for Biases in Learning Generalizations_\cite{mitchell1980need}, Tom M. Mitchell argues that inductive biases constitute the heart of generalization and indeed a key basis for learning itself:
        
        <blockquote>
            If consistency with the training instances is taken as the sole determiner of appropriate generalizations, then a program can never make the inductive leap necessary to classify instances beyond those it has observed. Only if the program has other sources of information, or biases for choosing one generalization over the other, can it non-arbitrarily classify instances beyond those in the training set....
    
            The impact of using a biased generalization language is clear: each subset of instances for which there is no expressible generalization is a concept that could be presented to the program, but which the program will be unable to describe and therefore unable to learn. If it is possible to know ahead of time that certain subsets of instances are irrelevant, then it may be useful to leave these out of the generalization language, in order to simplify the learning problem. ...

             Although removing all biases from a generalization system may seem to be a desirable goal, in fact the result is nearly useless. An unbiased learning system’s ability to classify new instances is no better than if it simply stored all the training instances and performed a lookup when asked to classify a subsequent instance.
            <footer>Tom M. Mitchell, “The Need for Biases in Learning Generalizations</footer>
          </blockquote>

        A key challenge of machine learning, therefore, is to design systems whose inductive biases align with the structure of the problem at hand. The effect of such efforts is not merely to endow the model with the capacity to learn key patterns, but also -- somewhat paradoxically -- to deliberately hamper the capacity of the model to learn other (presumably less useful) patterns, or at least to drive the model away from learning them. In other words, inductive biases stipulate the properties that we believe our model should have in order to generalize to future data; they thus encode our key assumptions about the problem itself.

        The machine learning toolkit has a wide array of methods to induce inductive biases in learning systems. For example, regularization methods \cite{girosi1995regularization} such as L1-/L2-penalties \cite{tibshirani1996regression}, dropout \cite{srivastava2014dropout}, or early stopping \cite{prechelt1998early} are a simple yet powerful means to impose Occam's razor onto the training process. By the same token, the maximum margin loss of support vector machines \cite{cortes1995support}, or model selection based on cross-validation can be described as inductive biases. Bayesian methods of almost any form induce inductive biases by placing explicit prior probabilities over model parameters. Machine learning systems that build on symbolic logic, such as inductive logic programming \cite{muggleton1991inductive}, encode established knowledge into very strict inductive biases, by forcing algorithms to reason about training examples explicitly in terms of hypotheses derived from pre-specified databases of facts. As nicely synthesized in Battaglia et al\cite{battaglia2018relational}, the standard layer types of modern neural networks each have distinct invariances that induce corresponding _relational inductive biases_; for example, convolutional layers have spatial translational invariance and induce a relational inductive bias of locality, whereas recurrent layers have a temporal invariance that induces the inductive bias of sequentiality. Such relational inductive biases are extremely powerful when well-matched to the data on which they are applied. 

          In the next section, I will introduce the neural representation learning framework -- the dominant paradigm of machine learning today -- and discuss inductive biases in this setting, with a special emphasis on recent tools for infusing external knowledge into the inductive biases of our models.
        
      </section>

      <section>
        <h2 id="RepLearning">Learned Representations of Data and Knowledge</h2>

        The performance of most information processing systems, including machine learning systems, typically depends heavily upon the data representations (or features) they employ. Historically, this meant the devotion of significant labor and expertise to _feature engineering_, the design of data transformations and preprocessing techniques to extract and organize discriminative features from data prior to the application of ML. _Representation learning_\cite{bengio2013representation,goodfellow2016representation} is an alternative to feature engineering, and refers to the training of learned representations of data (or knowledge graphs \cite{bordes2013translating}) that are optimized for utility in downstream tasks such as prediction or information retrieval.

        <h3 id="RepLearning_BG">Background on Representation Learning</h2>

        Many canonical methods in statistical learning can be considered representation learning methods. For example, low-dimensional data representations with desirable properties are learned by unsupervised methods such as principal components analysis \cite{pearson1901liii}, k-means clustering \cite{forgy1965cluster}, independent components analysis \cite{jutten1991blind}, and manifold learning methods such as Isomap \cite{tenenbaum2000global} and locally-linear embeddings \cite{roweis2000nonlinear}. Within the field of machine learning, the most popular paradigm for representation learning are neural networks\cite{bengio2013representation,goodfellow2016representation}, which provide an extremely flexible framework that can in theory be used to approximate any continuous function \cite{cybenko1989approximation}. Over the past two decades, representation learning with neural networks has steadily outperformed traditional feature engineering methods on a large family of tasks, including speech recognition \cite{dahl2010phone}, image processing \cite{hinton2006fast}, and natural language processing \cite{mikolov2011empirical}.

        A common feature of all the representation learning methods just mentioned is that they are designed to learn data representations that have lower dimensionality than the original data. This basic inductive bias is motivated by the so-called _manifold hypothesis_, which states that most real world data -- images, text, genomes, etc. -- are captured and stored in high dimensions but actually consist of some lower-dimensional data manifold embedded in that high-dimensional space.

        Another desirable property of learned representations is that they be _distributed representations_\cite{bengio2013representation,goodfellow2016representation}, composed of multiple elements that can be set separately from each other. Distributed representations are highly expressive: $n$ learned features with $k$ values can represent $k^n$ different concepts, with each feature element representing a degree of meaning along its own axis. This results in a rich similarity space that improves the generalizability of resultant models. The benefits of distributed representations apply to any data type, but are particularly obvious from a conceptual level when considering settings such as natural language processing \cite{mikolov2013distributed}, where the initial data representation are encoded as symbols that lack any relationship with their underlying meaning. For example, the two sentences (or their equivalent triples, in a knowledge graph setting) `ibuprofen impairs renal function` and `Advil damages the kidneys' have zero tokens or ngrams in common. Thus, machine learning programs based only on symbols would be unable to extrapolate from one sentence to the other without relying upon explicit mappings such as `ibuprofen has_name Advil', `impairs has_synonym damages', etc. In contrast, the distributed representations of these sentences should, in principle, be nearly identical, facilitating direct extrapolation.

        Over the past decade, neural networks have established themselves as the _de facto_ approach to representation learning for essentially every ML problem in which their training has been shown feasible \cite{bengio2013representation,goodfellow2016representation}. While some neural architectures -- e.g. Word2vec\cite{mikolov2013efficient} --  are designed exclusively to produce embeddings that will be utilized in downstream tasks, the primary appeal of neural networks is that _every_ deep learning architecture serves as a representation learning system. More specifically, the activations of each layer of neurons serves as a distributed representation of the input that is progressively refined in a hierarchical manner to produce representations of increased abstraction with increasing depth<label for="sn-in-his-later-books" class="margin-toggle sidenote-number"></label></span><input type="checkbox" id="sn-in-his-later-books" class="margin-toggle"/><span class="sidenote">While even single-layer neural networks can provably approximate any continuous function, this guarantee is impractical because the proof assumes an infinite number of hidden nodes\cite{cybenko1989approximation}. Deep neural networks, in contrast, allow for feature re-use that is exponential in the number of layers, which makes deep networks more expressive and more statistically efficient to train \cite{haastad1991power,bengio2013representation}</span>. In this light, a typical supervised neural network architecture of depth $k$, for example, can arguably be best understood as a representation learning architecture of depth $k-1$ followed by a simple linear or logistic regression (see Figure \ref{fig:paradigms_knowledge_nn}A).

        Representations learned by neural networks have a number of desirable properties. First, neural representations are low-dimensional, distributed, and hierarchically organized, as described above. Neural networks have the ability to learn parameterized mappings that are strongly nonlinear but can still be used to directly compute embeddings for new data points. Yoshuo Bengio and others have extensively argued that neural networks have a higher capacity for generalization versus other well-established ML methods such as kernels \cite{bengio2005non,bengio2006curse} and decision trees \cite{bengio2010decision}, specifically because they avoid an excessively strong inductive bias towards _smoothness_; in other words, when making a new prediction for some new data point $x$, deep representation learning methods do not exclusively rely upon the training points that are immediately nearby $x$ in the original feature space. 

        Representation learning using neural networks also benefits from being modular, and therefore flexible<label for="sn-in-his-later-books" class="margin-toggle sidenote-number"></label></span><input type="checkbox" id="sn-in-his-later-books" class="margin-toggle"/><span class="sidenote">The flexibility of neural networks doesn't come without a price: In addition to obvious concerns about highly parameterized models and overfitting\cite{friedman2001elements}, for example, the ease of implementing complicated DL architectures has arguably produced a research culture focused on ever-larger -- and more costly\cite{lacoste2019quantifying} -- models that are often poorly characterised and very difficult to reproduce \cite{lipton2018troubling.}</span> and extensible to design. For example, given two neural architectures that each create a distributed representation of a unique data modality, these can be straightforwardly combined into a single, fused architecture that creates a composite multi-modal representation (e.g. combining audio embeddings and visual embeddings into composite video embeddings\cite{ngiam2011multimodal}). Such an approach is leveraged in Chapter~\ref{chapter:chemGeAlign}. Another example of the power afforded by the modularity of neural architectures are _Generative Adversarial Networks_ (GANs) \cite{goodfellow2014generative}, which learn to generate richly structured data by pitting a data-simulating `generator' model against a jointly-trained `discrimator' model that is optimized to distinguish real from generated data. In Supplemental Chapter~\ref{chapter:fracgan}, I demonstrate this approach using a GAN trained to simulate hip radiographs.

        Taken together, neural architectures can be designed to expressively implement a broad array of inductive biases, while still allowing the network parameters to search over millions of compatible functions.

        <h3 id="RepLearning_Domain">Infusing Domain Knowledge into Neural Representations</h2>

        Neural networks have largely absolved the contemporary researcher of the need to hand-engineer features, but this reality has not eliminated the role of external knowledge in ML design. In this section, I compare and contrast the various methods for explicitly and implicitly infusing domain knowledge into neural representations.

        The first paradigm involves the design of layers and architectures that align the representational capacity of the network with our prior knowledge of the problem domain (Figure \ref{fig:paradigms_knowledge_nn} top left). For instance, if we know that the data we provide have a particular property (e.g. unordered features), we can enforce corresponding constraints in our architecture (e.g. permutation invariance, as in DeepSet \cite{zhang2019deep} or self-attention\cite{vaswani2017attention} without position encodings). This is an example of a relational inductive bias \cite{battaglia2018relational}. Relatedly, we can manually wire the network in a manner that corresponds with our prior understanding of relationships between variables. Peng et al \cite{peng2019combining} adopted this approach by building a feed forward neural network for single cell RNA-Seq data in which the input neurons for each gene were wired according to the Gene Ontology \cite{ashburner2000gene}; this approach strictly weakens the capacity of the network, but may be useful if we have a strong prior that particular relationships would be confounding. An alternative means to a similar end is to perform graph convolutions over edges that reflect domain knowledge \cite{mcdermott2019deep}.

        Another explicit paradigm for infusing knowledge into neural networks is to augment the architecture with the ability to query external information (Figure \ref{fig:paradigms_knowledge_nn} top right). For example, models can be augmented with knowledge graphs in the form of fact triples, which they can query using an attention mechanism \cite{annervaz2018learning, kishimoto2018knowledge}. More generally, attention can be used to allow modules to incorporate relevant information from embeddings of any knowledge source or data modality. For example, \citet{xu2015show} introduced an architecture in which a language model attends to images to generate image captions. Self-attention, or intra-attention, is an attention mechanism that allows for relating different positions within a single sequence \cite{cheng2016long, vaswani2017attention}, image \cite{parmar2018image}, or other instance of input data; this allows representations to better share and synthesis information across features.

        Transfer learning\cite{yang200610, pan2009survey} provides a family of methods to infuse knowledge into a learning algorithm that has been gained from a previous learning task. This is related to, but distinct from _multi-task learning_, which seeks to learn several tasks simultaneously under the premise that performance and efficiency can be improved by sharing knowledge between the tasks during learning. While there are many forms of transfer learning \cite{zhuang2019comprehensive}, the canonical form in the setting of deep learning is _pretraining_ (Figure \ref{fig:paradigms_knowledge_nn} bottom left). In pretraining, model weights from a trained neural network are used to initialize some subset of the weights in another network; these parameters can then be either frozen or ``fine-tuned" with further training on a the target task. Initial transfer learning experiments were conducted using unsupervised pretraining with autoencoders 
        <label for="sn-in-his-later-books" class="margin-toggle sidenote-number"></label></span><input type="checkbox" id="sn-in-his-later-books" class="margin-toggle"/><span class="sidenote">_Autoencoders_ \cite{hinton2006reducing} learn representations guided by the inductive bias that a good representation should be able to be used to reconstruct its raw input. They are an example of an `encoder-decoder' architecture, which consist of an encoder, which take the raw input and use a series of layers to embed it into a low-dimensional space, and a decoder, which takes an embedding from the encoder and tries to construct raw data; this combined architecture is then trained in an end-to-end fashion. When the decoder is trained specifically to reconstruct the exact same input passed into the encoder, this is called an autoencoder. (Alternatively, decoders can be trained to produce related data, a prominent example being Seq2seq models that can, for example, encode a sentence from one language and decode it into another\cite{sutskever2014sequence}.) _Variational autoencoders_ \cite{kingma2013auto} combine autoencoding with stochastic variational inference to build generative models that can be use for sampling entirely new data.</span> before transferring weights to a supervised model for a downstream task; this technique is an example of inductive _semi-supervised learning_\cite{van2020survey}. In the past decade, supervised pretraining has become very popular, with the quintessential example being the initialization of an image processing architecture with all but the final layer of a model trained on the ImageNet dataset \cite{deng2009imagenet}. More recently, _self-supervised_ transfer learning has received significant attention, particularly in natural language processing. In self-supervised learning, subsets of a data or feature set are masked, and neural networks are trained to predict them from remaining features. The resulting representations can then be used directly for downstream tasks, such as information retrieval, or be leveraged for transfer learning. Word embeddings \cite{mikolov2013distributed} are arguably the first widespread instance of self-supervised transfer learning, with more recent methods including language model pretraining \cite{howard2018universal, devlin2018bert, vaswani2017attention}.

      _Contrastive learning_ methods (Figure \ref{fig:paradigms_knowledge_nn} bottom right) learn representations by taking in small sets of examples and optimize embeddings to bring similar data together while driving dissimilar data apart. This is a form of _metric learning_. Early methods in this field include Siamese \cite{koch2015siamese, filzen2017representing} and Triplet networks \cite{hoffer2015deep}, which were initially developed to learn deep representations of images. Recent analyses suggest that many methods developed in the past several years have failed to advance beyond triplet networks \cite{musgrave2020metric}. Contrastive methods have been used in the pretraining step of a semi-supervised framework to achieve the current state-of-the-art in limited data image classification \cite{chen2020simple}. In addition, contrastive optimization can be leveraged using multi-modal data to create aligned representations across modalities \cite{deng2018triplet}.

      The methods described in this section can be described as a spectrum. Hand-engineered architectures are based on strong and specific prior assumptions about the problem domain, and are used to fundamentally alter the _representational capacity_ of the network. In contrast, self-supervised and contrastive architectures make very minimal specific assumptions about the problem domain, and do nothing to alter the representational capacity of the algorithm; instead their innovation lies in devising _training schemes and loss functions_ that will guide the network to learn underlying relationships and find a generalizable solution. In between these two extremes, augmenting networks with access to external knowledge through attention mechanisms often make the assumption that specific knowledge will be helpful, but allow the model to determine for itself which knowledge to employ. Transfer learning makes the assumption that other specific learning tasks will provide useful knowledge and experience for the target domain, but makes minimal assumptions about precisely what this knowledge would be. Despite (arguably significant) philosophical differences, these and yet other paradigms are not mutually exclusive, and share the common goal of improving generalization and data efficiency by introducing richer domain understanding into the neural networks.

      Finally, while this section -- and indeed several chapters of this thesis -- focuses on the design of neural architectures and training curricula, the role of domain knowledge is truly inescapable when it comes to the evaluation of deployable systems. Accordingly, the topic of deployment analysis will also be a major theme of this thesis.


      </section>
