@article{goodfellow2014,
author = {Goodfellow, Ian and Shlens, Jonathon and Szegedy, Christian},
year = {2014},
month = {12},
pages = {},
title = {Explaining and Harnessing Adversarial Examples},
booktitle = {arXiv 1412.6572}
}

@article{alexnet,
author = {Krizhevsky, Alex and Sutskever, Ilya and E. Hinton, Geoffrey},
year = {2012},
month = {01},
pages = {},
title = {ImageNet Classification with Deep Convolutional Neural Networks},
volume = {25},
booktitle = {Neural Information Processing Systems}
}

@article{Carlini2017,
abstract = {Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.},
archivePrefix = {arXiv},
arxivId = {1705.07263},
author = {Carlini, Nicholas and Wagner, David},
doi = {10.1145/3128572.3140444},
eprint = {1705.07263},
file = {:C$\backslash$:/Users/anast/Dropbox (MIT)/work/Mendeley/Carlini{\_}AdversarialExamplesAreNotEasilyDetected{\_}Arxiv2017.pdf:pdf},
isbn = {978-1-4503-5202-4},
journal = {arXiv},
title = {{Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods}},
url = {http://arxiv.org/abs/1705.07263},
year = {2017}
}
@article{Szegedy2013,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {1312.6199},
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
doi = {10.1021/ct2009208},
eprint = {1312.6199},
file = {:C$\backslash$:/Users/anast/Dropbox (MIT)/work/Mendeley/Szegedy{\_}IntriguingPropertiesNeuralNetworks{\_}Arxiv2014.pdf:pdf},
isbn = {1549-9618},
issn = {15499618},
journal = {arXiv},
pages = {1--10},
pmid = {22545027},
title = {{Intriguing properties of neural networks}},
url = {http://arxiv.org/abs/1312.6199},
year = {2014}
}

@article{Madry2017,
abstract = {Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
archivePrefix = {arXiv},
arxivId = {1706.06083},
author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
doi = {10.1227/01.NEU.0000255452.20602.C9},
eprint = {1706.06083},
file = {:C$\backslash$:/Users/anast/Dropbox (MIT)/work/Mendeley/Madry{\_}TowardsDeepModelResistantToAdversarialAttacks{\_}Arxiv2017.pdf:pdf},
isbn = {9781405161251},
issn = {1607-551X},
journal = {arXiv},
pages = {1--27},
pmid = {17460516},
title = {{Towards Deep Learning Models Resistant to Adversarial Attacks}},
url = {http://arxiv.org/abs/1706.06083},
year = {2017}
}
@article{Kurakin2016,
abstract = {Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.},
archivePrefix = {arXiv},
arxivId = {1607.02533},
author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
eprint = {1607.02533},
file = {:C$\backslash$:/Users/anast/Dropbox (MIT)/work/Mendeley/Kurakin{\_}AdversarialExamplesPhysicalWorld{\_}ICLR2017.pdf:pdf},
journal = {ICLR},
pages = {1--14},
title = {{Adversarial examples in the physical world}},
url = {http://arxiv.org/abs/1607.02533},
year = {2017}
}
@article{Biggio2017,
abstract = {Learning-based pattern classifiers, including deep networks, have demonstrated impressive performance in several application domains, ranging from computer vision to computer security. However, it has also been shown that adversarial input perturbations carefully crafted either at training or at test time can easily subvert their predictions. The vulnerability of machine learning to adversarial inputs (also known as adversarial examples), along with the design of suitable countermeasures, have been investigated in the research field of adversarial machine learning. In this work, we provide a thorough overview of the evolution of this interdisciplinary research area over the last ten years, starting from pioneering, earlier work up to more recent work aimed at understanding the security properties of deep learning algorithms, in the context of different applications. We report interesting connections between these apparently-different lines of work, highlighting common misconceptions related to the evaluation of the security of machine-learning algorithms. We finally discuss the main limitations of current work, along with the corresponding future research challenges towards the design of more secure learning algorithms.},
archivePrefix = {arXiv},
arxivId = {1712.03141},
author = {Biggio, Battista and Roli, Fabio},
eprint = {1712.03141},
file = {:C$\backslash$:/Users/anast/Dropbox (MIT)/work/Mendeley/Biggio{\_}TenYearsAfterRistAdversarialML{\_}2017.pdf:pdf},
journal = {arXiv},
keywords = {adversarial examples,adversarial machine learning,deep,evasion attacks,poisoning attacks,secure learning},
pages = {32--37},
title = {{Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning}},
url = {http://arxiv.org/abs/1712.03141},
year = {2017}
}
@article{Athalye2018,
abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. For each of the three types of obfuscated gradients we discover, we describe characteristic behaviors of defenses exhibiting this effect and develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 8 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely and 1 partially.},
archivePrefix = {arXiv},
arxivId = {1802.00420},
author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
eprint = {1802.00420},
file = {:C$\backslash$:/Users/anast/Dropbox (MIT)/work/Mendeley/Athalye{\_}ObfuscatedGradientsGiveFalseSenseSecurity{\_}Arxiv2018.pdf:pdf},
journal = {arXiv},
title = {{Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples}},
url = {http://arxiv.org/abs/1802.00420},
year = {2018}
}
@article{Athalye2017a,
abstract = {Neural network-based classifiers parallel or exceed human-level accuracy on many common tasks and are used in practical systems. Yet, neural networks are susceptible to adversarial examples, carefully perturbed inputs that cause networks to misbehave in arbitrarily chosen ways. When generated with standard methods, these examples do not consistently fool a classifier in the physical world due to viewpoint shifts, camera noise, and other natural transformations. Adversarial examples generated using standard techniques require complete control over direct input to the classifier, which is impossible in many real-world systems. We introduce the first method for constructing real-world 3D objects that consistently fool a neural network across a wide distribution of angles and viewpoints. We present a general-purpose algorithm for generating adversarial examples that are robust across any chosen distribution of transformations. We demonstrate its application in two dimensions, producing adversarial images that are robust to noise, distortion, and affine transformation. Finally, we apply the algorithm to produce arbitrary physical 3D-printed adversarial objects, demonstrating that our approach works end-to-end in the real world. Our results show that adversarial examples are a practical concern for real-world systems.},
archivePrefix = {arXiv},
arxivId = {1707.07397},
author = {Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
eprint = {1707.07397},
file = {:C$\backslash$:/Users/anast/Dropbox (MIT)/work/Mendeley/Athalye{\_}SynthesizingRobustAdversarialExamples{\_}ICLR2018.pdf:pdf},
journal = {ICLR},
title = {{Synthesizing Robust Adversarial Examples}},
url = {http://arxiv.org/abs/1707.07397},
year = {2018}
}
@article{Carlini2017a,
abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input {\$}x{\$} and any target classification {\$}t{\$}, it is possible to find a new input {\$}x'{\$} that is similar to {\$}x{\$} but classified as {\$}t{\$}. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from {\$}95\backslash{\%}{\$} to {\$}0.5\backslash{\%}{\$}. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with {\$}100\backslash{\%}{\$} probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
archivePrefix = {arXiv},
arxivId = {1608.04644},
author = {Carlini, Nicholas and Wagner, David},
doi = {10.1109/SP.2017.49},
eprint = {1608.04644},
file = {:C$\backslash$:/Users/anast/Dropbox (MIT)/work/Mendeley/Carlini{\_}EvaluatingRobustnessNN{\_}Arxiv2017.pdf:pdf},
isbn = {9781509055326},
issn = {10816011},
journal = {Proceedings - IEEE Symposium on Security and Privacy},
pages = {39--57},
pmid = {7546524},
title = {{Towards Evaluating the Robustness of Neural Networks}},
year = {2017}
}
@article{Engstrom2017,
abstract = {We show that simple transformations, namely translations and rotations alone, are sufficient to fool neural network-based vision models on a significant fraction of inputs. This is in sharp contrast to previous work that relied on more complicated optimization approaches that are unlikely to appear outside of a truly adversarial setting. Moreover, fooling rotations and translations are easy to find and require only a few black-box queries to the target model. Overall, our findings emphasize the need for designing robust classifiers even in natural, benign contexts.},
archivePrefix = {arXiv},
arxivId = {1712.02779},
author = {Engstrom, Logan and Tran, Brandon and Tsipras, Dimitris and Schmidt, Ludwig and Madry, Aleksander},
eprint = {1712.02779},
file = {:C$\backslash$:/Users/anast/Dropbox (MIT)/work/Mendeley/Engstrom{\_}Madry{\_}RotationTranslationSufficeFoolingCNNs{\_}Arxiv2018.pdf:pdf},
journal = {arXiv},
title = {{A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations}},
url = {http://arxiv.org/abs/1712.02779},
year = {2018}
}

@article{Tramer2017,
abstract = {Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model. In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large ({\~{}}25) dimensionality. Adversarial subspaces with higher dimensionality are more likely to intersect. We find that for two different models, a significant fraction of their subspaces is shared, thus enabling transferability. In the first quantitative analysis of the similarity of different models' decision boundaries, we show that these boundaries are actually close in arbitrary directions, whether adversarial or benign. We conclude by formally studying the limits of transferability. We derive (1) sufficient conditions on the data distribution that imply transferability for simple model classes and (2) examples of scenarios in which transfer does not occur. These findings indicate that it may be possible to design defenses against transfer-based attacks, even for models that are vulnerable to direct attacks.},
archivePrefix = {arXiv},
arxivId = {1704.03453},
author = {Tram{\`{e}}r, Florian and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
eprint = {1704.03453},
file = {:C$\backslash$:/Users/anast/Dropbox (MIT)/work/Mendeley/Tramer{\_}SpaceTransferableAdversarialExamples{\_}Arxiv2017.pdf:pdf},
journal = {arXiv},
pages = {1--15},
title = {{The Space of Transferable Adversarial Examples}},
url = {http://arxiv.org/abs/1704.03453},
year = {2017}
}
