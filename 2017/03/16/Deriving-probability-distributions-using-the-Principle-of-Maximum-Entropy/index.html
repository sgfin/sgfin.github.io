<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Deriving probability distributions using the Principle of Maximum Entropy</title>
  <meta name="description" content="Uniform, gaussian, exponential, and another distribution all from first principles.">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: [
      "MathMenu.js",
      "MathZoom.js",
      "AssistiveMML.js",
      "a11y/accessibility-menu.js"
    ],
    jax: ["input/TeX", "output/CommonHTML"],
    TeX: {
      extensions: [
        "AMSmath.js",
        "AMSsymbols.js",
        "noErrors.js",
        "noUndefined.js",
      ]
    }
  });
</script>

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-122144402-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-122144402-1');
    </script>

 <!--   <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
  

  <link rel="stylesheet" type="text/css" href="/css/tufte.css">
  <!-- <link rel="stylesheet" type="text/css" href="/css/print.css" media="print"> -->

  <link rel="canonical" href="http://sgfin.github.io/2017/03/16/Deriving-probability-distributions-using-the-Principle-of-Maximum-Entropy/">

  <link rel="alternate" type="application/rss+xml" title="Machine Learning and Medicine" href="http://sgfin.github.io/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group" style="padding-bottom: 15px;">
	<!-- <a href="/"><img class="badge" src="/assets/img/badge_1.png" alt="SGF"></a> -->
	
		
  	
		
		    
		      <a href="/">About</a>
		    
	    
  	
		
		    
		      <a href="/css/print.css"></a>
		    
	    
  	
		
  	
		
		    
		      <a href="/posts/">posts</a>
		    
	    
  	
		
		    
		      <a href="/learning-resources/">ML Resources</a>
		    
	    
  	
		
		    
		      <a href="/genereviews_random/"></a>
		    
	    
  	
	</nav>
</header>
    <article class="group">
      <h1>Deriving probability distributions using the Principle of Maximum Entropy</h1>
<p class="subtitle">March 16, 2017</p>

<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a>    <ul>
      <li><a href="#maximum-entropy-principle" id="markdown-toc-maximum-entropy-principle">Maximum Entropy Principle</a></li>
      <li><a href="#lagrange-multipliers" id="markdown-toc-lagrange-multipliers">Lagrange Multipliers</a></li>
    </ul>
  </li>
  <li><a href="#1-derivation-of-maximum-entropy-probability-distribution-with-no-other-constraints-uniform-distribution" id="markdown-toc-1-derivation-of-maximum-entropy-probability-distribution-with-no-other-constraints-uniform-distribution">1. Derivation of maximum entropy probability distribution with no other constraints (uniform distribution)</a>    <ul>
      <li><a href="#satisfy--constraint" id="markdown-toc-satisfy--constraint">Satisfy  constraint</a></li>
      <li><a href="#putting-together" id="markdown-toc-putting-together">Putting Together</a></li>
    </ul>
  </li>
  <li><a href="#2-derivation-of-maximum-entropy-probability-distribution-for-given-fixed-mean-mu-and-variance-sigma2-gaussian-distribution" id="markdown-toc-2-derivation-of-maximum-entropy-probability-distribution-for-given-fixed-mean-mu-and-variance-sigma2-gaussian-distribution">2. Derivation of maximum entropy probability distribution for given fixed mean \(\mu\) and variance \(\sigma^{2}\) (gaussian distribution)</a>    <ul>
      <li><a href="#satisfy-first-constraint" id="markdown-toc-satisfy-first-constraint">Satisfy first constraint</a></li>
      <li><a href="#satisfy-second-constraint" id="markdown-toc-satisfy-second-constraint">Satisfy second constraint</a></li>
      <li><a href="#putting-together-1" id="markdown-toc-putting-together-1">Putting together</a></li>
    </ul>
  </li>
  <li><a href="#3-derivation-of-maximum-entropy-probability-distribution-of-half-bounded-random-variable-with-fixed-mean-barr-exponential-distribution" id="markdown-toc-3-derivation-of-maximum-entropy-probability-distribution-of-half-bounded-random-variable-with-fixed-mean-barr-exponential-distribution">3. Derivation of maximum entropy probability distribution of half-bounded random variable with fixed mean \(\bar{r}\) (exponential distribution)</a>    <ul>
      <li><a href="#satisfying-first-constraint" id="markdown-toc-satisfying-first-constraint">Satisfying first constraint</a></li>
      <li><a href="#satisfying-the-second-constraint" id="markdown-toc-satisfying-the-second-constraint">Satisfying the second constraint</a></li>
      <li><a href="#putting-together-2" id="markdown-toc-putting-together-2">Putting together</a></li>
    </ul>
  </li>
  <li><a href="#4-maximum-entropy-of-random-variable-over-range-r-with-set-of-constraints-leftlangle-f_nxrightrangle-alpha_n-with-n1dots-n-and-f_n-is-of-polynomial-order" id="markdown-toc-4-maximum-entropy-of-random-variable-over-range-r-with-set-of-constraints-leftlangle-f_nxrightrangle-alpha_n-with-n1dots-n-and-f_n-is-of-polynomial-order">4. Maximum entropy of random variable over range \(R\) with set of constraints \(\left\langle f_{n}(x)\right\rangle =\alpha_{n}\) with \(n=1\dots N\) and \(f_{n}\) is of polynomial order</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>In this post, I derive the uniform, gaussian, exponential, and another funky probability distribution from the first principles of information theory. I originally did it for a class, but I enjoyed it and learned a lot so I am adding it here so I don’t forget about it.</p>

<p>I actually think it’s pretty magical that these common distributions just pop out when you are using the information framework.  It feels so much more satisfying/intuitive than it did before.</p>

<h3 id="maximum-entropy-principle">Maximum Entropy Principle</h3>

<p>Recall that <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">information entropy</a> is a mathematical framework for quantifying “uncertainty.”  The formula for the information entropy of a random variable is
\(H(x) = - \int p(x)\ln p(x)dx\)
.  	
In statistics/information theory, the <a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution">maximum entropy probability distribution</a> is (you guessed it!) the distribution that, given any constraints, has maximum entropy.  Given a choice of distributions, the “Principle of Maximum Entropy” tells us that the maximum entropy distribution is the best.  Here’s a snippet of the idea from the <a href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy">wikipedia page</a>:</p>
<blockquote>
  <p>The principle of maximum entropy states that, subject to precisely stated prior data (such as a proposition that expresses testable information), the probability distribution which best represents the current state of knowledge is the one with largest entropy.
Another way of stating this: Take precisely stated prior data or testable information about a probability distribution function. Consider the set of all trial probability distributions that would encode the prior data. According to this principle, the distribution with maximal information entropy is the proper one.
…
In ordinary language, the principle of maximum entropy can be said to express a claim of epistemic modesty, or of maximum ignorance. The selected distribution is the one that makes the least claim to being informed beyond the stated prior data, that is to say the one that admits the most ignorance beyond the stated prior data.</p>
</blockquote>

<h3 id="lagrange-multipliers">Lagrange Multipliers</h3>

<p>Given the above, we can use the maximum entropy principle to derive the best probability distribution for a given use.  A useful tool in doing so is the Lagrange Multiplier (<a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint">Khan Acad article</a>, <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">wikipedia</a>), which helps us maximize or minimize a function under a given set of constraints.</p>

<p>For a single variable function \(f(x)\) subject to the constraint \(g(x) = c\), the lagrangian is of the form:
\(\mathcal{L}(x,\lambda) = f(x) - \lambda(g(x)- c)\)
, which is then differentiated and set to zero to find a solution.</p>

<p>The above can then be extended to additional variables and constraints as:</p>

\[\mathcal{L}(x_{1}\dots x_{n},\lambda_{1}\dots\lambda{n}) = f(x_{1}\dots x_{n}) - \Sigma_{k=1}^{M}\lambda_{k}g_{k}(x_{1}\dots x_{n})\]

<p>and solving</p>

\[\nabla x_{1},\dots,x_{n},\lambda_{1}\dots \lambda_{M}\mathcal{L}(x_{1}\dots x_{n},\lambda_{1}\dots\lambda{n})=0\]

<p>or, equivalently, solving</p>

\[\begin{cases}
\nabla f(x)-\Sigma_{K=1}^{M}\lambda_{k}\nabla g_{k}(x)=0\\
g_{1}(x)=\dots=g_{M}(x)=0
\end{cases}\]

<p>In this case, since we are deriving probability distributions, the integral of the pdf must sum to one, and as such, every derivation will include the constraint \((\int p(x)dx-1)=0\).</p>

<p>With all that, we can begin:</p>

<h2 id="1-derivation-of-maximum-entropy-probability-distribution-with-no-other-constraints-uniform-distribution">1. Derivation of maximum entropy probability distribution with no other constraints (uniform distribution)</h2>

<p>First, we solve for the case where the only constraint is that the distribution is a pdf, which we will see is the uniform distribution. To maximize entropy, we want to minimize the following function:</p>

\[J(p)=\int_{a}^{b} p(x)\ln p(x)dx-\lambda_{0}\left(\int_{a}^{b} p(x)dx-1\right)\]

<p>.  Taking the derivative with respect ot \(p(x)\) and setting to zero,</p>

\[\frac{\delta J}{\delta p(x)}=1+\ln p(x)-\lambda_{0}=0\]

\[\ln p(x)=1-\lambda_{0}\]

\[p(x)=e^{1 -\lambda_{0}}\]

<p>, which in turn must satisfy</p>

\[\int_{a}^{b} p(x)dx=1=\int_{a}^{b} e^{-\lambda_{0}+1}dx\]

<p>Note: To check if this is a minimum (which would maximize entropy given the
way the equation was set up), we also need to see if the second
derivative with respect to \(p(x)\) is positive here or not, which it
clearly always is:</p>

\[\frac{\delta J}{\delta p(x)^{2}dx}=\frac{1}{p(x)}\]

<h3 id="satisfy--constraint">Satisfy  constraint</h3>

\[\int_{a}^{b} p(x)dx=\int_{a}^{b} e^{1 -\lambda_{0}}dx=1\]

\[\int_{a}^{b} e^{-\lambda_{0}+1}dx=1\]

\[e^{-\lambda_{0}+1} \int_{a}^{b} dx=1\]

\[e^{-\lambda_{0}+1} (b-a) = 1\]

\[e^{-\lambda_{0}+1} = \frac{1}{b-a}\]

\[-\lambda_{0}+1 = \ln\frac{1}{b-a}\]

\[\lambda_{0} = 1 -\ln \frac{1}{b-a}\]

<h3 id="putting-together">Putting Together</h3>

<p>Plugging the constraint \(\lambda_{0} = 1 -\ln \frac{1}{b-a}\) into the pdf \(p(x)=e^{1 -\lambda_{0}}\), we have:</p>

\[p(x)=e^{1 -\lambda_{0}}\]

\[p(x)=e^{1 -(1 -\ln \frac{1}{b-a})}\]

\[p(x)=e^{1 -1 + \ln \frac{1}{b-a}}\]

\[p(x)=e^{\ln \frac{1}{b-a}}\]

\[p(x)=\frac{1}{b-a}\]

<p>.  Of course, this is only defined in the range between \(a\) and \(b\), however, so the final function is:</p>

\[p(x)=\begin{cases}
\frac{1}{b-a} &amp; a\leq x \leq b\\
0 &amp; \text{otherwise}
\end{cases}\]

<h2 id="2-derivation-of-maximum-entropy-probability-distribution-for-given-fixed-mean-mu-and-variance-sigma2-gaussian-distribution">2. Derivation of maximum entropy probability distribution for given fixed mean \(\mu\) and variance \(\sigma^{2}\) (gaussian distribution)</h2>

<p>Now, for the case when we have a specified mean and variance, which we will see is the gaussian distribution.  To maximize entropy, we want to minimize the following function:</p>

\[J(p)=\int p(x)\ln p(x)dx-\lambda_{0}\left(\int p(x)dx-1\right)-\lambda_{1}\left(\int p(x)(x-\mu)^{2}dx-\sigma^{2}\right)\]

<p>, where the first constraint is the definition of pdf and the second is the definition of the variance (which also gives us the mean for free).  Taking the derivative with respect ot p(x) and setting to zero,</p>

\[\frac{\delta J}{\delta p(x)}=1+\ln p(x)-\lambda_{0}-\lambda_{1}(x-\mu)^{2}=0\]

\[\ln p(x)=1-\lambda_{0}-\lambda_{1}(x-\mu)^{2}\]

\[p(x)=e^{-\lambda_{0}+1-\lambda_{1}(x-\mu)^{2}}\]

<p>, which in turn must satisfy</p>

\[\int p(x)dx=1=\int e^{-\lambda_{0}+1-\lambda_{1}(x-\mu)^{2}}dx\]

<p>and</p>

\[\int p(x)(x-\mu)^{2}dx=\sigma^{2}=\int e^{-\lambda_{0}+1-\lambda_{1}(x-\mu)^{2}}(x-\mu)^{2}dx\]

<p>Again, \(\frac{\delta J}{\delta p(x)^{2}dx}=\frac{1}{p(x)}\) is always positive, so our solution will be minimum.</p>

<h3 id="satisfy-first-constraint">Satisfy first constraint</h3>

\[\int p(x)dx=1=\int e^{-\lambda_{0}+1-\lambda_{1}(x-\mu)^{2}}dx\]

\[1=\int e^{-\lambda_{0}+1-\lambda_{1}z^{2}}dz\]

\[1=\int e^{-\lambda_{0}+1-\lambda_{1}z^{2}}dz\]

<p>\(1=\int e^{-\lambda_{0}+1}*e^{-\lambda_{1}z^{2}}dz\)
\(1=e^{-\lambda_{0}+1}\int e^{-\lambda_{1}z^{2}}dz\)
\(e^{\lambda_{0}-1}=\int e^{-\lambda_{1}z^{2}}dz\)
\(e^{\lambda_{0}-1}=\int e^{-\lambda_{1}z^{2}}dz\)
\(e^{\lambda_{0}-1}=\sqrt{\frac{\pi}{\lambda_{1}}}\)</p>

<h3 id="satisfy-second-constraint">Satisfy second constraint</h3>

\[\int p(x)(x-\mu)^{2}dx=\sigma^{2}=\int e^{-\lambda_{0}+1-\lambda_{1}(x-\mu)^{2}}(x-\mu)^{2}dx\]

\[\sigma^{2}=\int e^{-\lambda_{0}+1-\lambda_{1}(x-\mu)^{2}}(x-\mu)^{2}dx\]

\[\sigma^{2}=\int e^{-\lambda_{0}-1-\lambda_{1}z^{2}}z^{2}dz\]

\[\sigma^{2}e^{\lambda_{0}-1}=\int e^{-\lambda_{1}z^{2}}z^{2}dz\]

\[\sigma^{2}e^{\lambda_{0}-1}=\frac{1}{2}\sqrt{\frac{\pi}{\lambda_{1}^{3}}}\]

\[\sigma^{2}e^{\lambda_{0}-1}=\frac{1}{2\lambda_{1}}\sqrt{\frac{\pi}{\lambda_{1}}}\]

\[2\lambda_{1}\sigma^{2}e^{\lambda_{0}-1}=\sqrt{\frac{\pi}{\lambda_{1}}}\]

<h3 id="putting-together-1">Putting together</h3>

\[\sqrt{\frac{\pi}{\lambda_{1}}}=e^{\lambda_{0}-1}=2\lambda_{1}\sigma^{2}e^{\lambda_{0}-1}\]

<p>so</p>

\[e^{\lambda_{0}-1}=2\lambda_{1}\sigma^{2}e^{\lambda_{0}-1}\]

\[1=2\lambda_{1}\sigma^{2}\]

\[\lambda_{1}=\frac{1}{2\sigma^{2}}\]

<p>. Plugging in for the other lambda,</p>

\[\sqrt{\frac{\pi}{\lambda_{1}}}=e^{\lambda_{0}-1}\]

\[\sqrt{2\sigma^{2}\pi}=e^{\lambda_{0}-1}\]

\[\ln\sqrt{2\sigma^{2}\pi}=\lambda_{0}-1\]

\[\lambda_{0}=\ln\sqrt{2\sigma^{2}\pi}+1\]

<p>Now, we plug back into the first equation</p>

\[p(x)=e^{-\lambda_{0}-1-\lambda_{1}(x-\mu)^{2}}\]

\[=e^{-\ln\sqrt{2\sigma^{2}\pi}-\frac{1}{2\sigma^{2}}(x-\mu)^{2}}\]

\[=e^{-\ln\sqrt{2\sigma^{2}\pi}}e^{-\frac{1}{2\sigma^{2}}(x-\mu)^{2}}\]

\[=\frac{1}{\sqrt{2\pi\sigma^{2}}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}\]

<p>which we can note is, by definition, the pdf of the Gaussian!</p>

<h2 id="3-derivation-of-maximum-entropy-probability-distribution-of-half-bounded-random-variable-with-fixed-mean-barr-exponential-distribution">3. Derivation of maximum entropy probability distribution of half-bounded random variable with fixed mean \(\bar{r}\) (exponential distribution)</h2>

<p>Now, constrain on a fixed mean, but no fixed variance, which we will see is the exponential distribution. To maximize entropy, we want to minimize the following function:</p>

\[J(p)=\int p(x)\ln p(x)dx-\lambda_{0}\left(\int_{0}^{\infty}p(x)dx-1\right)-\lambda\left(\int_{0}^{\infty}x*p(x)dx-\bar{r}\right)\]

<p>Now take derivative</p>

\[\frac{\delta J}{\delta p(x)dx}=1+\ln p(x)-\lambda_{0}-\lambda_{1}x\]

<p>To check if this is a minimum of the function, we need to see if the
second derivative is positive with respect to p(x), which it is:</p>

<p>\(\frac{\delta J}{\delta p(x)^{2}dx}=\frac{1}{p(x)}\) Setting the first
derivative to zero, we have</p>

\[0=1+\ln p(x)-\lambda_{0}-\lambda_{1}x\]

\[p(x)=e^{-\lambda_{0}+1+-\lambda x}\]

<p>, which must satisfy the constaints \(\int_{0}^{\infty}p(x)dx=1\) and
\(\int_{0}^{\infty}x*p(x)dx-\bar{r}\).</p>

<h3 id="satisfying-first-constraint">Satisfying first constraint</h3>

\[\int_{0}^{\infty}p(x)dx=1\]

\[\int_{0}^{\infty}e^{-\lambda_{0}+1-\lambda_{1}x}dx=1\]

\[\int_{0}^{\infty}e^{-\lambda_{1}x}dx=e^{\lambda_{0}-1}\]

\[\frac{1}{\lambda_{1}}=e^{\lambda_{0}+1}\]

\[\lambda_{1}=e^{-\lambda_{0}+1}\]

<h3 id="satisfying-the-second-constraint">Satisfying the second constraint</h3>

\[\int_{0}^{\infty}x*e^{-\lambda_{0}+1-\lambda_{1}x}dx=\bar{r}\]

\[\int_{0}^{\infty}x*e^{-\lambda_{0}+1}e^{\lambda_{1}x}dx=\bar{r}\]

<p>substituting in \(\lambda_{1}=e^{-\lambda_{0}+1}\) from above</p>

\[\int_{0}^{\infty}x*\lambda_{1}e^{\lambda_{1}x}dx=\bar{r}\]

<h3 id="putting-together-2">Putting together</h3>

<p>Rather than evaluating this last integral above, we can simply stop and
note that in evaluating our constraints we have stumbled upon the
formula for an exponential random variable with parameter \(\lambda\)!</p>

<p>More explicitly:</p>

\[\int_{0}^{\infty}x*\lambda_{1}e^{\lambda_{1}x}dx=\bar{r}\]

\[\int_{0}^{\infty}x*p(x)dx=\bar{r}\]

<p>where \(p(x)=\lambda e^{\lambda x}\), the pdf of the exponential function
for \(x\ge0\), where \(\lambda=\frac{1}{\bar{r}}\).</p>

<p>In other words,</p>

\[p(x)=\begin{cases}
\frac{1}{\bar{r}}e^{-\frac{x}{\bar{r}}} &amp; x\ge0\\
0 &amp; x&lt;0
\end{cases}\]

<h2 id="4-maximum-entropy-of-random-variable-over-range-r-with-set-of-constraints-leftlangle-f_nxrightrangle-alpha_n-with-n1dots-n-and-f_n-is-of-polynomial-order">4. Maximum entropy of random variable over range \(R\) with set of constraints \(\left\langle f_{n}(x)\right\rangle =\alpha_{n}\) with \(n=1\dots N\) and \(f_{n}\) is of polynomial order</h2>

<p>\(f_{n}\) must be even order for all enforced constraints.</p>

<p>Following the same approach as above:</p>

\[J(p)=-\int p(x)\ln p(x)dx+\lambda_{0}\left(\int p(x)dx-1\right)+\Sigma_{i=1}^{N}\lambda_{i}\left(p(x)f_{i}(x)dx-a_{i}\right)\]

\[\frac{\delta J}{\delta p(x)dx}=-1-\ln p(x)+\lambda_{0}+\Sigma_{i=1}^{N}\lambda_{i}f_{i}(x)\]

\[0=-1-\ln p(x)+\lambda_{0}+\Sigma_{i=1}^{N}\lambda_{i}f_{i}(x)\]

\[p(x)=e^{\lambda_{0}-1+\Sigma_{i=1}^{N}\lambda_{i}f_{i}(x)}\]

<p>all where \(f_{i}(x)=\Sigma_{j=1}^{M}b_{j}x^{j}\).</p>

<p>We now consider the conditions in which the random variable can be
defined in the entire domain \((-\infty,\infty)\). Looking at the
normalization constraint,</p>

\[\int p(x)dx=\int e^{\lambda_{0}-1+\Sigma_{i=1}^{N}\lambda_{i}f_{i}(x)}dx=1\]

<p>we note that we need our exponential function to integrate to 1. In
order for this equation to be defined in the entire real domain, we thus
will need the exponential function to integrate to a finite value, so
that we can provide a normalization constant that will result in
integration to 1.</p>

<p>Looking at the function
\(e^{\lambda_{0}-1+\Sigma_{i=1}^{N}\lambda_{i}f_{i}(x)}\) (which must
remain finite for all x), we can thus conclude that
\(\lambda_{0}-1+\Sigma_{i=1}^{N}\lambda_{i}f_{i}(x)\) must not converge to
positive infinity, but may converge to negative infinity (because it
would cause the exponential to converge to zero) or to any finite value
as \(x\) approaches positive or negative infinity. The only components of
this function that depend on \(x\) are the polynomail constraints of form
\(f_{i}(x)=\Sigma_{j=1}^{M}b_{j}x^{j}\). As such, these constraints are
the only components at risk to force the function towards infinity,
provided that \(\lambda_{0}\neq\infty.\) Therefore, because the
\(\lambda_{i}\) corresponding to can any \(f_{i}\) can be positive or
negative, the function will be able to be defined so long
\(f_{i}(x)=\Sigma_{j=1}^{M}b_{j}x^{j}&lt;\infty\) for all \(x\), or
\(f_{i}(x)=\Sigma_{j=1}^{M}b_{j}x^{j}&gt;-\infty\) for all \(x.\)</p>

<p>Finally, we can consider the conditions for which these criteria for
\(f_{i}\) will be satisfied. In short, the only way to guarantee that
\(f_{i}\) remain either positive for negative will be if the dominant
component of the polynomial \(f_{i}\) is of an EVEN order for all \(i\) s.t.
\(\lambda_{i}\neq0\). If the dominant component is odd, then \(f_{i}\) will
either move from negative infinity to positive infinity (or, if negated,
from positive infinity to negative infinity) as x moves across the
domain, which means that no finite and nonzero \(\lambda_{i}\) could be
chosen to maintain the criteria outlined above.</p>



    </article>
    <span class="print-footer">Deriving probability distributions using the Principle of Maximum Entropy - March 16, 2017 - Sam Finlayson</span>
    <footer>
  <hr class="slender">
  <ul class="footer-links">
    <li><a href="mailto:samuel_finlayson@hms.harvard.edu"><span class="icon-mail3"></span></a></li>    
    
      <li>
        <a href="//www.twitter.com/IAmSamFin"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//github.com/sgfin"><span class="icon-github"></span></a>
      </li>
    
      <li>
        <a href="/feed"><span class="icon-rss2"></span></a>
      </li>
      
  </ul>
<div class="credits">
<span>&copy; 2025 &nbsp;&nbsp;SAM FINLAYSON</span></br> <br>
<span>This site created with the <a href="//github.com/clayh53/tufte-jekyll">Tufte theme for  </a> in <a href="//jekyllrb.com">Jekyll</a>.</span> 
</div>  
</footer>
  </body>
</html>
